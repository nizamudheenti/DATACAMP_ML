{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHAPTER 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas library import pandas as pd\n",
    "#load csv file using read_csv function of pandas\n",
    "df = pd.read_csv(path_to_csv_file)\n",
    "print first 5 rows\n",
    "print(df.head())\n",
    "print columns of the dataframe print(df.columns)\n",
    "print column datatypes of the dataframe print(df.dtypes)\n",
    "#select integer datatype columns \n",
    "only_ints = df.select_dtypes(include=['int'])\n",
    "print(only_ints.columns)\n",
    "#convert category column into label columns \n",
    "pd.get_dummies(df, columns=['Country'],prefix='C')\n",
    "\n",
    "pd.get_dummies(df, columns=['Country'],drop_first=True, prefix='C')\n",
    "#counting the labels \n",
    "counts = df['Country'].value_counts()\n",
    "print(counts)\n",
    "#creating mask for labels that have less than 5 count as others \n",
    "mask = df['Country'].isin(counts[counts < 5].index)\n",
    "df['Country'][mask] = 'Other'\n",
    "print(pd.value_counts(colors))\n",
    "\n",
    "\n",
    "df['Binary_Violation'] = 0\n",
    "df.loc[df['Number_of_Violations'] > 0,'Binary_Violation'] = 1\n",
    "\n",
    "import numpy as np\n",
    "df['Binned_Group'] = pd.cut(\n",
    "df['Number_of_Violations'],bins=[-np.inf, 0, 2, np.inf],\n",
    "labels=[1, 2, 3]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHAPTER 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())\n",
    "#checking for null \n",
    "print(df.isnull())\n",
    "#check the count of null value in a specific column \n",
    "print(df['StackOverflowJobsRecommend'].isnull().sum())\n",
    "#printing notnull \n",
    "print(df.notnull())\n",
    "# Drop all rows with at least one missing values\n",
    "df.dropna(how='any')\n",
    "# Drop rows with missing values in a specific column\n",
    "df.dropna(subset=['VersionControl'])\n",
    "# Replace missing values in a specific column\n",
    "# with a given string\n",
    "df['VersionControl'].fillna(value='None Given', inplace=True)\n",
    "# Record where the values are not missing\n",
    "df['SalaryGiven'] = df['ConvertedSalary'].notnull()\n",
    "# Drop a specific column\n",
    "df.drop(columns=['ConvertedSalary'])\n",
    "#checking mean median of the specific column \n",
    "print(df['ConvertedSalary'].mean())\n",
    "print(df['ConvertedSalary'].median())\n",
    "#replacing nan values with mean of the specific column\n",
    "df['ConvertedSalary'] = df['ConvertedSalary'].fillna(df['ConvertedSalary'].mean())\n",
    "#converting column datatype as int64 using astype function\n",
    "df['ConvertedSalary'] = df['ConvertedSalary'].astype('int64')\n",
    "#replacing nan values with rounded values of mean\n",
    "df['ConvertedSalary'] = df['ConvertedSalary'].fillna(round(df['ConvertedSalary'].mean()))\n",
    "#then checking the datatype of rawsalary column\n",
    "print(df['RawSalary'].dtype)\n",
    "#printing the rawsalary column's first 5 rows\n",
    "print(df['RawSalary'].head())\n",
    "#replacing the \",\" with \"\" using replace function\n",
    "df['RawSalary'] = df['RawSalary'].str.replace(',', '')\n",
    "#converting its datatype as float using astype function\n",
    "df['RawSalary'] = df['RawSalary'].astype('float')\n",
    "#converting to numerical values \n",
    "coerced_vals = pd.to_numeric(df['RawSalary'],errors='coerce')\n",
    "print(df[coerced_vals.isna()].head())\n",
    "#in python we can apply following function calls using chain methon\n",
    "df['column_name'] = df['column_name'].method1()\n",
    "df['column_name'] = df['column_name'].method2()\n",
    "df['column_name'] = df['column_name'].method3()\n",
    "df['column_name'] = df['column_name'].method1().method2().method3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHAPTER 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib library for ploting \n",
    "import matplotlib as plt\n",
    "#draw histogram of the dataframe\n",
    "df.hist()\n",
    "#show the graph\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#ploting boxplot of the 'column_1' feature of the dataframe \n",
    "df[['column_1']].boxplot()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#importing seaborn for plotting\n",
    "import seaborn as sns\n",
    "#plotting pairplot of the dataframe\n",
    "sns.pairplot(df)\n",
    "df.describe()\n",
    "\n",
    "\n",
    "\n",
    "#importing minmaxscaler from sklearn's preprocessing module\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "#initializing minmaxscaler\n",
    "scaler = MinMaxScaler()\n",
    "#fiting the minmax scaler using age data of the dataframe\n",
    "scaler.fit(df[['Age']])\n",
    "#transforming the data into a new column of dataframe\n",
    "df['normalized_age'] = scaler.transform(df[['Age']])\n",
    "#IMPORTING STANDARSCALER FROM SKLEARN.PREPROCESSING\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#INITIALISING THE STANDARD SCALER\n",
    "scaler = StandardScaler()\n",
    "#FITTING WITH DATA\n",
    "scaler.fit(df[['Age']])\n",
    "#TRANSFORMING GIVEN DATA\n",
    "df['standardized_col'] = scaler.transform(df[['Age']])\n",
    "#IMPORTING POWER TRANSFORMER FROM SKLEARN.PREPROCESSING MODULE\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "#INITIALISING THE POWERTRANSFORMER\n",
    "log = PowerTransformer()\n",
    "#FITTING THE POWERTRANSFORMER WITH DATA\n",
    "log.fit(df[['ConvertedSalary']])\n",
    "#TRANSFORMING DATA USING POWER TRANSFORMER\n",
    "df['log_ConvertedSalary'] =log.transform(df[['ConvertedSalary']])\n",
    "\n",
    "#CEATING A CUTOFF\n",
    "q_cutoff = df['col_name'].quantile(0.95)\n",
    "#SELECTING THE VALUES THAT LESS THAN THE CUTOFF\n",
    "mask = df['col_name'] < q_cutoff\n",
    "trimmed_df = df[mask]\n",
    "#SELECTING MEAN AND STANDAR DEVIATION\n",
    "mean = df['col_name'].mean()\n",
    "std = df['col_name'].std()\n",
    "#SELECT CUTOFF AS THREE TIMES STD\n",
    "cut_off = std * 3\n",
    "#INITIALISING UPPER AND LOWER BOND\n",
    "lower, upper = mean - cut_off, mean + cut_off\n",
    "#SELECTING NEW DATAS WITHOUT HAVING LESS THAN LOWER BOND VALUE AND HIGHER THAN UPPER BOND VALUES\n",
    "new_df = df[(df['col_name'] < upper) &(df['col_name'] > lower)]\n",
    "#INITIALISE THE STANDARDSCALER\n",
    "scaler = StandardScaler()\n",
    "#FITTING WITH DATA\n",
    "scaler.fit(train[['col']])\n",
    "#TRANSFORMING DATA\n",
    "train['scaled_col'] = scaler.transform(train[['col']])\n",
    "# FIT SOME MODEL\n",
    "test = pd.read_csv('test_csv')\n",
    "#TRANSFORMING DATA\n",
    "test['scaled_col'] = scaler.transform(test[['col']])\n",
    "#CHECKING MEAN\n",
    "train_mean = train[['col']].mean()\n",
    "#CHECKING STANDARD DEVIATION\n",
    "train_std = train[['col']].std()\n",
    "cut_off = train_std * 3\n",
    "train_lower = train_mean - cut_off\n",
    "train_upper = train_mean + cut_off\n",
    "# Subset train data\n",
    "test = pd.read_csv('test_csv')\n",
    "# Subset test data\n",
    "test = test[(test[['col']] < train_upper) &(test[['col']] > train_lower)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHAPTER 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(speech_df.head())\n",
    "\n",
    "speech_df['text'] = speech_df['text'].str.replace('[^a-zA-Z]', ' ')\n",
    "\n",
    "speech_df['text'] = speech_df['text'].str.lower()\n",
    "print(speech_df['text'][0])\n",
    "\n",
    "speech_df['char_cnt'] = speech_df['text'].str.len()\n",
    "print(speech_df['char_cnt'].head())\n",
    "\n",
    "speech_df['word_cnt'] =speech_df['text'].str.split()\n",
    "speech_df['word_cnt'].head(1)\n",
    "\n",
    "speech_df['word_counts'] =speech_df['text'].str.split().str.len()\n",
    "print(speech_df['word_splits'].head())\n",
    "\n",
    "speech_df['avg_word_len'] =speech_df['char_cnt'] / speech_df['word_cnt']\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "print(cv)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(min_df=0.1, max_df=0.9)\n",
    "min_df : minimum fraction of documents the word must occur in\n",
    "max_df : maximum fraction of documents the word can occur in\n",
    "\n",
    "cv.fit(speech_df['text_clean'])\n",
    "\n",
    "cv_transformed = cv.transform(speech_df['text_clean'])\n",
    "print(cv_transformed)\n",
    "\n",
    "cv_transformed.toarray()\n",
    "\n",
    "feature_names = cv.get_feature_names()\n",
    "print(feature_names)\n",
    "\n",
    "cv_transformed = cv.fit_transform(speech_df['text_clean'])\n",
    "print(cv_transformed)\n",
    "\n",
    "cv_df = pd.DataFrame(cv_transformed.toarray(),\n",
    "columns=cv.get_feature_names()).add_prefix('Counts_')\n",
    "print(cv_df.head())\n",
    "\n",
    "speech_df = pd.concat([speech_df, cv_df],axis=1, sort=False)\n",
    "print(speech_df.shape)\n",
    "\n",
    "print(speech_df['Counts_the'].head())\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tv = TfidfVectorizer()\n",
    "print(tv)\n",
    "\n",
    "tv = TfidfVectorizer(max_features=100,\n",
    "stop_words='english')\n",
    "max_features : Maximum number of columns created from TF-IDF\n",
    "stop_words : List of common words to omit e.g. \"and\", \"the\" etc.\n",
    "\n",
    "tv.fit(train_speech_df['text'])\n",
    "train_tv_transformed = tv.transform(train_speech_df['text']\n",
    "\n",
    "train_tv_df = pd.DataFrame(train_tv_transformed.toarray(),\n",
    "columns=tv.get_feature_names())\\\n",
    ".add_prefix('TFIDF_')\n",
    "train_speech_df = pd.concat([train_speech_df, train_tv_df],\n",
    "axis=1, sort=False)\n",
    "\n",
    "examine_row = train_tv_df.iloc[0]\n",
    "print(examine_row.sort_values(ascending=False))\n",
    "\n",
    "test_tv_transformed = tv.transform(test_df['text_clean'])\n",
    "test_tv_df = pd.DataFrame(test_tv_transformed.toarray(),\n",
    "columns=tv.get_feature_names())\\\n",
    ".add_prefix('TFIDF_')\n",
    "test_speech_df = pd.concat([test_speech_df, test_tv_df],\n",
    "axis=1, sort=False)\n",
    "\n",
    "tv_bi_gram_vec = TfidfVectorizer(ngram_range = (2,2))\n",
    "# Fit and apply bigram vectorizer\n",
    "tv_bi_gram = tv_bi_gram_vec\\\n",
    ".fit_transform(speech_df['text'])\n",
    "# Print the bigram features\n",
    "print(tv_bi_gram_vec.get_feature_names())\n",
    "\n",
    "# Create a DataFrame with the Counts features\n",
    "tv_df = pd.DataFrame(tv_bi_gram.toarray(),\n",
    "columns=tv_bi_gram_vec.get_feature_names())\\\n",
    ".add_prefix('Counts_')\n",
    "tv_sums = tv_df.sum()\n",
    "print(tv_sums.head())\n",
    "\n",
    "print(tv_sums.sort_values(ascending=False)).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
