{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimize(np.square,0).x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a loss function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimizing a loss function\n",
    "\n",
    "In this exercise you'll implement linear regression \"from scratch\" using scipy.optimize.minimize.\n",
    "\n",
    "We'll train a model on the Boston housing price data set, which is already loaded into the variables X and y. For simplicity, we won't include an intercept in our regression model.\n",
    "\n",
    "    Fill in the loss function for least squares linear regression.\n",
    "    Print out the coefficients from fitting sklearn's LinearRegression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The squared error, summed over training examples\n",
    "def my_loss(w):\n",
    "    s = 0\n",
    "    for i in range(y.size):\n",
    "        # Get the true and predicted target values for example 'i'\n",
    "        y_i_true = y[i]\n",
    "        y_i_pred = w@X[i]\n",
    "        s = s + (y_i_pred-y_i_true)**2\n",
    "    return s\n",
    "\n",
    "# Returns the w that makes my_loss(w) smallest\n",
    "w_fit = minimize(my_loss, X[0]).x\n",
    "print(w_fit)\n",
    "\n",
    "# Compare with scikit-learn's LinearRegression coefficients\n",
    "lr = LinearRegression(fit_intercept=False).fit(X,y)\n",
    "print(lr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the logistic and hinge losses\n",
    "\n",
    "In this exercise you'll create a plot of the logistic and hinge losses using their mathematical expressions, which are provided to you.\n",
    "\n",
    "The loss function diagram from the video is shown on the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the log_loss() and hinge_loss() functions at the grid points so that they are plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical functions for logistic and hinge losses\n",
    "def log_loss(raw_model_output):\n",
    "   return np.log(1+np.exp(-raw_model_output))\n",
    "def hinge_loss(raw_model_output):\n",
    "   return np.maximum(0,1-raw_model_output)\n",
    "\n",
    "# Create a grid of values and plot\n",
    "grid = np.linspace(-2,2,1000)\n",
    "plt.plot(grid, log_loss(grid), label='logistic')\n",
    "plt.plot(grid, hinge_loss(grid), label='hinge')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing logistic regression\n",
    "\n",
    "This is very similar to the earlier exercise where you implemented linear regression \"from scratch\" using scipy.optimize.minimize. However, this time we'll minimize the logistic loss and compare with scikit-learn's LogisticRegression (we've set C to a large value to disable regularization; more on this in Chapter 3!).\n",
    "\n",
    "The log_loss() function from the previous exercise is already defined in your environment, and the sklearn breast cancer prediction dataset (first 10 features, standardized) is loaded into the variables X and y.\n",
    "\n",
    "    Input the number of training examples into range().\n",
    "    Fill in the loss function for logistic regression.\n",
    "    Compare the coefficients to sklearn's LogisticRegression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The logistic loss, summed over training examples\n",
    "def my_loss(w):\n",
    "    s = 0\n",
    "    for i in range(y.size):\n",
    "        raw_model_output = w@X[i]\n",
    "        s = s + log_loss(raw_model_output * y[i])\n",
    "    return s\n",
    "\n",
    "# Returns the w that makes my_loss(w) smallest\n",
    "w_fit = minimize(my_loss, X[0]).x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized logistic regression\n",
    "\n",
    "In Chapter 1, you used logistic regression on the handwritten digits data set. Here, we'll explore the effect of L2 regularization.\n",
    "\n",
    "The handwritten digits dataset is already loaded, split, and stored in the variables X_train, y_train, X_valid, and y_valid. The variables train_errs and valid_errs are already initialized as empty lists.\n",
    "\n",
    "    Loop over the different values of C_value, creating and fitting a LogisticRegression model each time.\n",
    "    Save the error on the training set and the validation set for each model.\n",
    "    Create a plot of the training and testing error as a function of the regularization parameter, C.\n",
    "    Looking at the plot, what's the best value of C?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and validaton errors initialized as empty list\n",
    "train_errs = list()\n",
    "valid_errs = list()\n",
    "\n",
    "# Loop over values of C_value\n",
    "for C_value in [0.001, 0.01, 0.1, 1, 10, 100, 1000]:\n",
    "    # Create LogisticRegression object and fit\n",
    "    lr = LogisticRegression(C=C_value)\n",
    "    lr.fit(X_train,y_train)\n",
    "    \n",
    "    # Evaluate error rates and append to lists\n",
    "    train_errs.append( 1.0 - lr.score(X_train,y_train) )\n",
    "    valid_errs.append( 1.0 - lr.score(X_valid,y_valid) )\n",
    "    \n",
    "# Plot results\n",
    "plt.semilogx(C_values, train_errs, C_values, valid_errs)\n",
    "plt.legend((\"train\", \"validation\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression and feature selection\n",
    "\n",
    "In this exercise we'll perform feature selection on the movie review sentiment data set using L1 regularization. The features and targets are already loaded for you in X_train and y_train.\n",
    "\n",
    "We'll search for the best value of C using scikit-learn's GridSearchCV(), which was covered in the prerequisite course.\n",
    "\n",
    "    Instantiate a logistic regression object that uses L1 regularization.\n",
    "    Find the value of C that minimizes cross-validation error.\n",
    "    Print out the number of selected features for this value of C.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify L1 regularization\n",
    "lr = LogisticRegression(penalty='l1')\n",
    "\n",
    "# Instantiate the GridSearchCV object and run the search\n",
    "searcher = GridSearchCV(lr, {'C':[0.001, 0.01, 0.1, 1, 10]})\n",
    "searcher.fit(X_train, y_train)\n",
    "\n",
    "# Report the best parameters\n",
    "print(\"Best CV params\", searcher.best_params_)\n",
    "\n",
    "# Find the number of nonzero coefficients (selected features)\n",
    "best_lr = searcher.best_estimator_\n",
    "coefs = best_lr.coef_\n",
    "print(\"Total number of features:\", coefs.size)\n",
    "print(\"Number of selected features:\", np.count_nonzero(coefs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization and probabilities\n",
    "\n",
    "In this exercise, you will observe the effects of changing the regularization strength on the predicted probabilities.\n",
    "\n",
    "A 2D binary classification dataset is already loaded into the environment as X and y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    Compute the maximum predicted probability.\n",
    "    Run the provided code and take a look at the plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the regularization strength\n",
    "model = LogisticRegression(C=1)\n",
    "\n",
    "# Fit and plot\n",
    "model.fit(X,y)\n",
    "plot_classifier(X,y,model,proba=True)\n",
    "\n",
    "# Predict probabilities on training points\n",
    "prob = model.predict_proba(X)\n",
    "print(\"Maximum predicted probability\", prob.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing easy and difficult examples\n",
    "\n",
    "In this exercise, you'll visualize the examples that the logistic regression model is most and least confident about by looking at the largest and smallest predicted probabilities.\n",
    "\n",
    "The handwritten digits dataset is already loaded into the variables X and y. The show_digit function takes in an integer index and plots the corresponding image, with some extra information displayed above the image.\n",
    "#### Instructions\n",
    "#### 100 XP\n",
    "\n",
    "    Fill in the first blank with the index of the digit that the model is most confident about.\n",
    "    Fill in the second blank with the index of the digit that the model is least confident about.\n",
    "    Observe the images: do you agree that the first one is less ambiguous than the second?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X,y)\n",
    "\n",
    "# Get predicted probabilities\n",
    "proba = lr.predict_proba(X)\n",
    "\n",
    "# Sort the example indices by their maximum probability\n",
    "proba_inds = np.argsort(np.max(proba,axis=1))\n",
    "\n",
    "# Show the most confident (least ambiguous) digit\n",
    "show_digit(proba_inds[0], lr)\n",
    "\n",
    "# Show the least confident (most ambiguous) digit\n",
    "show_digit(proba_inds[-1], lr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
